{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "env_file = \"Tennis_Windows_x86_64\\Tennis.exe\" #'Tennis_Linux/Tennis.x86_64'#\n",
    "env = UnityEnvironment(file_name=env_file, no_graphics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device:  cpu\n",
      "NO grad. clipping used.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import progressbar as pb\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import OUprocess, Reply_buffer\n",
    "from Tennis_agent import SelfPlay_Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "states shape:  (2, 24)\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      "The state for the second agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.4669857  -1.5\n",
      "  0.          0.         -6.83172083  6.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('states shape: ',states.shape)\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print('The state for the second agent looks like:', states[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |#                                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 20: -0.004999999888241291\n",
      "Score (max over agents) from episode 20: 0.0\n",
      "Rolling average score: 0.004500000085681677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |##                                                       |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 40: -0.004999999888241291\n",
      "Score (max over agents) from episode 40: 0.0\n",
      "Rolling average score: 0.009750000154599547\n",
      "Actor loss: 0.08149810135364532 | Critic loss: 1.738916034810245e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   6% |###                                                      |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 60: -0.004999999888241291\n",
      "Score (max over agents) from episode 60: 0.0\n",
      "Rolling average score: 0.0081666667945683\n",
      "Actor loss: 0.07704493403434753 | Critic loss: 3.185501554980874e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   8% |####                                                     |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 80: -0.004999999888241291\n",
      "Score (max over agents) from episode 80: 0.0\n",
      "Rolling average score: 0.008625000133179128\n",
      "Actor loss: 0.07537087798118591 | Critic loss: 2.8992890293011442e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |#####                                                    |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 100: -0.004999999888241291\n",
      "Score (max over agents) from episode 100: 0.0\n",
      "Rolling average score: 0.00980000015348196\n",
      "Actor loss: 0.07332167029380798 | Critic loss: 9.574408977641724e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |######                                                   |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 120: -0.004999999888241291\n",
      "Score (max over agents) from episode 120: 0.0\n",
      "Rolling average score: 0.00980000015348196\n",
      "Actor loss: 0.07218969613313675 | Critic loss: 1.892745785880834e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |#######                                                  |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 140: -0.004999999888241291\n",
      "Score (max over agents) from episode 140: 0.0\n",
      "Rolling average score: 0.007700000125914812\n",
      "Actor loss: 0.06839580088853836 | Critic loss: 9.476395462115761e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  16% |#########                                                |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 160: -0.004999999888241291\n",
      "Score (max over agents) from episode 160: 0.0\n",
      "Rolling average score: 0.008600000143051147\n",
      "Actor loss: 0.06688211858272552 | Critic loss: 1.5626948879798874e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  18% |##########                                               |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 180: -0.004999999888241291\n",
      "Score (max over agents) from episode 180: 0.0\n",
      "Rolling average score: 0.010500000175088644\n",
      "Actor loss: 0.06528855860233307 | Critic loss: 1.3931613466411363e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |###########                                              |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 200: -0.004999999888241291\n",
      "Score (max over agents) from episode 200: 0.0\n",
      "Rolling average score: 0.008600000143051147\n",
      "Actor loss: 0.0627952367067337 | Critic loss: 1.5244241694745142e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  22% |############                                             |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 220: -0.004999999888241291\n",
      "Score (max over agents) from episode 220: 0.0\n",
      "Rolling average score: 0.007700000125914812\n",
      "Actor loss: 0.062277089804410934 | Critic loss: 1.4741935046913568e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  24% |#############                                            |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 240: -0.004999999888241291\n",
      "Score (max over agents) from episode 240: 0.0\n",
      "Rolling average score: 0.00980000015348196\n",
      "Actor loss: 0.05833248794078827 | Critic loss: 2.6076751964865252e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  26% |##############                                           |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 260: -0.004999999888241291\n",
      "Score (max over agents) from episode 260: 0.0\n",
      "Rolling average score: 0.007900000121444463\n",
      "Actor loss: 0.05727531760931015 | Critic loss: 1.825673462008126e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  28% |###############                                          |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 280: -0.004999999888241291\n",
      "Score (max over agents) from episode 280: 0.0\n",
      "Rolling average score: 0.004000000059604645\n",
      "Actor loss: 0.056246042251586914 | Critic loss: 1.6839110685396008e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |#################                                        |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 300: -0.004999999888241291\n",
      "Score (max over agents) from episode 300: 0.0\n",
      "Rolling average score: 0.0030000000447034836\n",
      "Actor loss: 0.051376357674598694 | Critic loss: 2.6105932192876935e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  32% |##################                                       |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 320: -0.004999999888241291\n",
      "Score (max over agents) from episode 320: 0.0\n",
      "Rolling average score: 0.0030000000447034836\n",
      "Actor loss: 0.049073848873376846 | Critic loss: 5.108600453240797e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  34% |###################                                      |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 340: -0.004999999888241291\n",
      "Score (max over agents) from episode 340: 0.0\n",
      "Rolling average score: 0.0\n",
      "Actor loss: 0.04685238003730774 | Critic loss: 2.2771997464587912e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  36% |####################                                     |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 360: -0.004999999888241291\n",
      "Score (max over agents) from episode 360: 0.0\n",
      "Rolling average score: 0.0010000000149011613\n",
      "Actor loss: 0.04580720141530037 | Critic loss: 2.3010667064227164e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  38% |#####################                                    |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 380: -0.004999999888241291\n",
      "Score (max over agents) from episode 380: 0.0\n",
      "Rolling average score: 0.0010000000149011613\n",
      "Actor loss: 0.04332006350159645 | Critic loss: 1.9384755432838574e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |######################                                   |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 400: -0.004999999888241291\n",
      "Score (max over agents) from episode 400: 0.0\n",
      "Rolling average score: 0.0020000000298023225\n",
      "Actor loss: 0.040790002793073654 | Critic loss: 2.0556255549308844e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  42% |#######################                                  |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 420: -0.004999999888241291\n",
      "Score (max over agents) from episode 420: 0.0\n",
      "Rolling average score: 0.0029000000469386576\n",
      "Actor loss: 0.03810300678014755 | Critic loss: 2.1288647985784337e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  44% |#########################                                |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 440: -0.004999999888241291\n",
      "Score (max over agents) from episode 440: 0.0\n",
      "Rolling average score: 0.003800000064074993\n",
      "Actor loss: 0.03870466724038124 | Critic loss: 1.403039823344443e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  46% |##########################                               |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 460: -0.004999999888241291\n",
      "Score (max over agents) from episode 460: 0.0\n",
      "Rolling average score: 0.002800000049173832\n",
      "Actor loss: 0.03406047075986862 | Critic loss: 1.286021142732352e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  48% |###########################                              |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 480: -0.004999999888241291\n",
      "Score (max over agents) from episode 480: 0.0\n",
      "Rolling average score: 0.002800000049173832\n",
      "Actor loss: 0.03374606370925903 | Critic loss: 1.5073070244397968e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  50% |############################                             |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 500: -0.004999999888241291\n",
      "Score (max over agents) from episode 500: 0.0\n",
      "Rolling average score: 0.004700000081211329\n",
      "Actor loss: 0.02801002934575081 | Critic loss: 3.132274287054315e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  52% |#############################                            |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 520: -0.004999999888241291\n",
      "Score (max over agents) from episode 520: 0.0\n",
      "Rolling average score: 0.003800000064074993\n",
      "Actor loss: 0.028722865507006645 | Critic loss: 2.0397295884322375e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  54% |##############################                           |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 540: -0.004999999888241291\n",
      "Score (max over agents) from episode 540: 0.0\n",
      "Rolling average score: 0.0029000000469386576\n",
      "Actor loss: 0.026890160515904427 | Critic loss: 1.8102080503012985e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  56% |###############################                          |\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) episode 560: -0.004999999888241291\n",
      "Score (max over agents) from episode 560: 0.0\n",
      "Rolling average score: 0.0029000000469386576\n",
      "Actor loss: 0.02311968430876732 | Critic loss: 2.18865698116133e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f887d070d3c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0magent_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[0mcloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0maloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# Update target network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\git_repos\\DRL\\Udacity\\Collaboration\\Tennis_agent.py\u001b[0m in \u001b[0;36mupdate_actor\u001b[1;34m(self, minibatch, agent_i)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;31m############### Actor updater ########################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_states_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;31m#get actions of minibatch for claculating policy gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mactions_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_i\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\git_repos\\DRL\\Udacity\\Collaboration\\Tennis_agent.py\u001b[0m in \u001b[0;36mget_states_actions\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_states_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mactions_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\git_repos\\DRL\\Udacity\\Collaboration\\Tennis_agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_states_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mstate_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mactions_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_agent(env, nr_episodes, num_agents, seed): \n",
    "    \n",
    "    max_buffer = int(1.e+6)\n",
    "    \n",
    "    noise_decay = 1#0.999\n",
    "    noise_lvl = 0.2\n",
    "    \n",
    "    batch_size = 256\n",
    "    update_steps = 5\n",
    "    GD_steps = 4\n",
    "    lra=1.e-3\n",
    "    lrc=1.e-3\n",
    "    tau=1\n",
    "    discount=0.999\n",
    "    \n",
    "    print('### ===================================== seed {} ======================================= ###'.format(seed))\n",
    "    print('lra {}, lrc {}, noise_lvl {}, noise_decay {}, batch_size {}, update_steps {}, GD_steps {}, tau {}, gamma {}'.format(\n",
    "        lra, lrc, noise_lvl,noise_decay,batch_size,update_steps,GD_steps,tau,discount))\n",
    "    \n",
    "    buffer = Reply_buffer(max_buffer,seed=seed)\n",
    "    ou = OUprocess(num_agents, action_size, sigma=noise_lvl, seed=seed)\n",
    "    agent = SelfPlay_Agent(state_size, action_size, num_agents, tau=tau, discount=discount, lr_act=lra, lr_crit=lrc, seed=seed)\n",
    "    print(agent.actor)\n",
    "    print(agent.critic)\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar()]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=nr_episodes).start()\n",
    "    ACloss = []\n",
    "    l_exists = False\n",
    "    max_avg = np.inf\n",
    "    e_score = np.zeros(nr_episodes)\n",
    "    rolling_avg = np.zeros(nr_episodes)\n",
    "    rolling_window = deque(maxlen=100)\n",
    "    t=0\n",
    "    for i in range(nr_episodes):                                         # play game for nr_episodes episodes\n",
    "        env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "        states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "        ou.reset()\n",
    "    #     ouB.reset()\n",
    "        scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "        while True:\n",
    "            t+=1\n",
    "            # select action\n",
    "            agent.actor.eval()\n",
    "            noise = ou.noise()\n",
    "            actA  = agent.act(states[np.newaxis,0]).data.cpu().numpy() + noise[0]\n",
    "            actB = agent.act(states[np.newaxis,1]).data.cpu().numpy() + noise[1]\n",
    "            actions = np.clip([actA[0], actB[0]],-1,1) # select an action (for each agent)\n",
    "            # execute and observe\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "            # store in replay buffer\n",
    "            samples = [states, np.asarray(actions), np.asarray(rewards), next_states, dones]\n",
    "            buffer.store(samples)\n",
    "            # roll over states to next time step\n",
    "            states = next_states                               \n",
    "            # keep track of scores\n",
    "            scores += env_info.rewards                         # update the score (for each agent)\n",
    "            # Update agents\n",
    "            if ((t-1) % update_steps == 0) and len(buffer.buffer) > batch_size:\n",
    "                l_exists = True\n",
    "                agent.actor.train()\n",
    "                for ep in range(GD_steps):\n",
    "                    minibatch = buffer.get_batch(batch_size)  # get batch of size (batch_size x 5) where the 2nd dim corresponds to (s,a,r,s',done)\n",
    "                    agent_i = np.random.choice(num_agents,len(minibatch))\n",
    "                    closs = agent.update_critic(minibatch,agent_i)\n",
    "                    aloss = agent.update_actor(minibatch,agent_i)\n",
    "\n",
    "            # Update target network\n",
    "            if agent.tau == 1 and t% int(2*update_steps) == 0:\n",
    "                agent.target_update()\n",
    "            else:\n",
    "                agent.target_update()\n",
    "            # exit loop if episode finished\n",
    "            if np.any(dones): \n",
    "                break\n",
    "\n",
    "        if l_exists:\n",
    "            ACloss.append([aloss,closs])\n",
    "\n",
    "        ou.sigma *= noise_decay\n",
    "\n",
    "        e_score[i] = np.max(scores)\n",
    "        rolling_window.append(e_score[i])\n",
    "        rolling_avg[i] = np.mean(np.array(rolling_window))\n",
    "        if rolling_avg[i] > 0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i, rolling_avg[i]))\n",
    "            # Save actor and critic\n",
    "            torch.save(agent.actor.state_dict(), 'Tennis.player')\n",
    "            torch.save(agent.critic.state_dict(), 'Tennis.critic')\n",
    "            max_avg = rolling_avg[i]\n",
    "            print('Networks saved')\n",
    "\n",
    "        if (i+1) % 20 == 0:\n",
    "            print('Total score (averaged over agents) episode {}: {}'.format(i+1,np.mean(scores)))\n",
    "            print('Score (max over agents) from episode {}: {}'.format(i+1, e_score[i]))\n",
    "            print('Rolling average score: {}'.format(rolling_avg[i]))\n",
    "    #         print('Noise level: ',ouA.sigma,ouB.sigma)\n",
    "            if l_exists:\n",
    "                print(\"Actor loss: {} | Critic loss: {}\".format(*ACloss[-1]))\n",
    "            if rolling_avg[i]>max_avg:\n",
    "                # Save best actor and critic\n",
    "                torch.save(agent.actor.state_dict(), 'Tennis.player')\n",
    "                torch.save(agent.critic.state_dict(), 'Tennis.critic')\n",
    "                max_avg = rolling_avg[i]\n",
    "                print('Better network saved!')\n",
    "        # update progress widget bar\n",
    "        timer.update(i+1)\n",
    "\n",
    "    print('Max score: ', rolling_avg.max())\n",
    "    timer.finish()\n",
    "    \n",
    "    print('### ================================================================================== ###')\n",
    "    ACloss = np.asarray(ACloss)\n",
    "    return ACloss, rolling_avg, e_score, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_episodes = 20#1400\n",
    "for i in range(10):\n",
    "    seed = np.random.randint(10000000)\n",
    "    ACloss, rolling_avg, e_score, agent = train_agent(env, nr_episodes, num_agents, seed)\n",
    "    if rolling_avg.max() > 0.5:\n",
    "        print('Tennis solved!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(e_score)\n",
    "plt.plot(rolling_avg,'r')\n",
    "plt.plot(0.5*np.ones_like(rolling_avg),'k')\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(ACloss[:,0])\n",
    "plt.subplot(122)\n",
    "plt.plot(ACloss[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lra 0.0005, lrc 0.0005, noise_lvl 0.2, noise_decay 1, batch_size 128, update_steps 1, GD_steps 2 => 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_max = 2000\n",
    "agent.actor.eval()\n",
    "for i in range(5):                                         # play game for nr_episodes episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    ou.reset()\n",
    "#     ouB.reset()\n",
    "    while True:#t<T_max:\n",
    "        t+=1\n",
    "        # select action\n",
    "        noise = ou.noise()\n",
    "        actA  = agent.act(states[np.newaxis,0]).data.cpu().numpy() + noise[0]\n",
    "        actB = agent.act(states[np.newaxis,1]).data.cpu().numpy() + noise[1]\n",
    "        actions = np.clip([actA[0], actB[0]],-1,1) # select an action (for each agent)\n",
    "        # execute and observe\n",
    "        #actions = [np.array([0.1,1]),np.array([0.1,1])]\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        # roll over states to next time step\n",
    "        states = next_states                               \n",
    "        # keep track of scores\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        # exit loop if episode finished\n",
    "        if np.any(dones): \n",
    "            break\n",
    "    print('Best player score in episode {}: {}'.format(i,np.max(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
